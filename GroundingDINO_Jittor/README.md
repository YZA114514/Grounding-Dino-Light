# GroundingDINO Jittor Implementation

This project is a Jittor implementation of GroundingDINO, as part of the 2025 ANN Final Project (Tsinghua University).

**Paper**: [Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection](https://arxiv.org/abs/2303.05499)

**Code**: [https://github.com/YZA114514/Grounding-Dino-Light.git](https://github.com/YZA114514/Grounding-Dino-Light.git)

## ğŸ¯ Highlights

- âœ… **83%+ of original performance**: Achieved **21.4 AP** on LVIS minival (paper: 25.6 AP)
- âœ… **Pure Jittor implementation**: Multi-scale deformable attention, bi-directional cross-modal attention, BERT encoder
- âœ… **Complete training pipeline**: Zero-shot evaluation, fine-tuning, ablation studies
- âœ… **63% inference speedup**: Optimized from 14.8s to 5.5s per image

## ğŸ“Š Experimental Results

### Zero-Shot Detection (LVIS minival, 4752 images)

| Metric | Jittor (Ours) | Paper Target | Ratio |
|--------|--------------|--------------|-------|
| **AP** | 21.4% | 25.6% | 83.6% |
| **APâ‚…â‚€** | 28.7% | - | - |
| **APáµ£** (rare) | 12.7% | 14.4% | 88.2% |
| **APc** (common) | 18.9% | 19.6% | 96.4% |
| **APf** (frequent) | 25.3% | 32.2% | 78.6% |

### Comparison with OWL-ViT (LVIS minival)

| Method | AP | APâ‚…â‚€ | APâ‚› | APâ‚˜ | APâ‚— | Time/img |
|--------|-----|------|-----|-----|-----|----------|
| **Grounding DINO (Jittor)** | **21.4** | **28.7** | **13.7** | **30.5** | **39.1** | 5.5s |
| OWL-ViT | 17.9 | 28.2 | 9.1 | 24.0 | 35.7 | **2.4s** |
| **Î”** | **+3.5** | +0.5 | **+4.6** | **+6.5** | +3.4 | - |

**Key findings**:
- Grounding DINO outperforms OWL-ViT by **+3.5 AP** overall
- Significant advantage on small/medium objects (**+4.6 / +6.5 AP**) due to multi-scale feature fusion
- OWL-ViT is 2.3Ã— faster due to single-pass inference

### Fine-tuning Results (100-image subset evaluation)

| Method | AP | APâ‚…â‚€ | APáµ£ | APc | APf |
|--------|-----|------|-----|-----|-----|
| Zero-shot (Jittor) | 36.5 | 47.5 | 16.7 | 22.8 | 38.0 |
| Fine-tuned (640Â², 5ep, 1k samples) | **41.4** | **50.1** | **23.3** | **29.1** | **42.8** |
| *Improvement* | *+4.9* | *+2.6* | *+6.6* | *+6.3* | *+4.8* |

Fine-tuning with just 1k samples improves AP by **+4.9pp**, with rare categories benefiting the most (**+6.6pp**).

### Ablation Study (LVIS minival)

| Setting | AP | APâ‚…â‚€ | APáµ£ | APc | APf | Î”AP |
|---------|-----|------|-----|-----|-----|-----|
| Jittor Baseline | 21.4 | 28.7 | 12.7 | 18.9 | 25.3 | - |
| w/o text cross-attn | 8.5 | 12.6 | 6.1 | 7.5 | 9.9 | **-60%** |
| random text | 0.3 | 0.4 | 0.0 | 0.0 | 0.3 | **-99%** |

Removing text cross-attention drops AP by 60%, confirming cross-modal fusion is the core mechanism.

## ğŸ”§ Implementation Highlights

1. **Multi-Scale Deformable Attention**: Pure Jittor implementation using `grid_sample` for bilinear interpolation
2. **Pure Jittor BERT**: Complete BERT-base architecture compatible with HuggingFace weights
3. **Weight Mapping**: Handles `module.` prefix removal, `in_proj` splitting (Q/K/V), nested tensor wrappers
4. **JIT Compilation Fix**: Resolved multi-GPU resource contention via `JT_COMPILE_PARALLEL` limiting
5. **Category Batching**: 60 categories/batch (~215 tokens) to stay within BERT's 256 token limit

## Project Structure

```
GroundingDINO_Jittor/
â”œâ”€â”€ jittor_implementation/        # Core codebase
â”‚   â”œâ”€â”€ models/                   # Model architecture
â”‚   â”‚   â”œâ”€â”€ backbone/             # Swin Transformer
â”‚   â”‚   â”œâ”€â”€ attention/            # MS Deformable Attention
â”‚   â”‚   â”œâ”€â”€ transformer/          # Encoder & Decoder
â”‚   â”‚   â”œâ”€â”€ text_encoder/         # Pure Jittor BERT
â”‚   â”‚   â”œâ”€â”€ fusion/               # Bi-directional Cross-Attention
â”‚   â”‚   â”œâ”€â”€ query/                # Language-guided Query Selection
â”‚   â”‚   â””â”€â”€ groundingdino.py      # Full model assembly
â”‚   â”œâ”€â”€ data/                     # Data loading & transforms
â”‚   â”œâ”€â”€ losses/                   # Focal, GIoU, L1, Grounding losses
â”‚   â”œâ”€â”€ eval/                     # LVIS evaluator
â”‚   â””â”€â”€ train/                    # Training pipeline
â”œâ”€â”€ scripts/                      # Utility scripts
â”‚   â”œâ”€â”€ eval_lvis_zeroshot_full.py  # Official zero-shot evaluation
â”‚   â”œâ”€â”€ finetune_lvis_full.py       # LVIS fine-tuning
â”‚   â”œâ”€â”€ eval_owlvit_lvis.py         # OWL-ViT comparison
â”‚   â”œâ”€â”€ eval_Gdino_ablation.py      # Ablation experiments
â”‚   â””â”€â”€ convert_weights_pytorch_to_jittor.py
â””â”€â”€ weights/                      # Model checkpoints
```

## Installation

### Prerequisites
- Python 3.9
- CUDA 11.x (for GPU acceleration)
- PyTorch (for weight conversion only)

### Step 1: Clone and Setup Environment

```bash
# Clone repository
git clone https://github.com/YZA114514/Grounding-Dino-Light.git
cd Grounding-Dino-Light

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
# or: .venv\Scripts\activate  # Windows

# Install dependencies
cd GroundingDINO_Jittor
pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple

# Verify installation
python -c "import jittor as jt; print(f'Jittor: {jt.__version__}')"
```

### Step 2: Download and Setup BERT Model

BERT æ¨¡å‹ç”¨äºæ–‡æœ¬ç¼–ç ï¼Œéœ€è¦ä» HuggingFace ä¸‹è½½ `bert-base-uncased`ï¼š

```bash
# æ–¹æ³•1: ä½¿ç”¨ transformers è‡ªåŠ¨ä¸‹è½½ï¼ˆæ¨èï¼‰
python -c "from transformers import AutoTokenizer, AutoModel; \
    AutoTokenizer.from_pretrained('bert-base-uncased'); \
    AutoModel.from_pretrained('bert-base-uncased')"

# æ–¹æ³•2: æ‰‹åŠ¨ä¸‹è½½å¹¶æ”¾ç½®åˆ°æŒ‡å®šç›®å½•
# ä¸‹è½½åœ°å€: https://huggingface.co/bert-base-uncased
# æ”¾ç½®ä½ç½®: GroundingDINO_Jittor/models/bert-base-uncased/
#   - config.json
#   - vocab.txt
#   - pytorch_model.bin (æˆ– model.safetensors)
```

ä¸‹è½½å®Œæˆåï¼Œè®¾ç½®ç¦»çº¿æ¨¡å¼ä»¥åŠ é€Ÿæ¨ç†ï¼š
```bash
export HF_HUB_OFFLINE=1  # Linux/Mac
# æˆ– Windows: set HF_HUB_OFFLINE=1
```

### Step 3: Download and Convert Official Weights

ä»å®˜æ–¹ä»“åº“ä¸‹è½½ PyTorch é¢„è®­ç»ƒæƒé‡ï¼Œå¹¶è½¬æ¢ä¸º Jittor æ ¼å¼ï¼š

```bash
# åˆ›å»ºæƒé‡ç›®å½•
mkdir -p weights && cd weights

# ä¸‹è½½ Swin-T å®˜æ–¹æƒé‡ (~694MB)
wget https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth

# è¿”å›é¡¹ç›®ç›®å½•
cd ..

# è½¬æ¢ä¸º Jittor æ ¼å¼ (éœ€è¦å®‰è£… PyTorch)
python scripts/convert_weights_pytorch_to_jittor.py \
    --pytorch_weight weights/groundingdino_swint_ogc.pth \
    --output weights/groundingdino_swint_ogc_jittor.pkl \
    --verify
```

è½¬æ¢è„šæœ¬åŠŸèƒ½ï¼š
- åŠ è½½ PyTorch æƒé‡ (.pth)
- å¤„ç†æƒé‡åç§°æ˜ å°„ï¼ˆç§»é™¤ `module.` å‰ç¼€ã€æ‹†åˆ† `in_proj` æƒé‡ï¼‰
- è½¬æ¢ä¸º Jittor æ ¼å¼ (.pkl)
- éªŒè¯è½¬æ¢æ­£ç¡®æ€§

### Step 4: Download LVIS Dataset

LVIS æ•°æ®é›†ç”¨äºé›¶æ ·æœ¬è¯„ä¼°å’Œå¾®è°ƒï¼š

```bash
# åˆ›å»ºæ•°æ®ç›®å½•
mkdir -p ../LVIS/minival

# ä¸‹è½½ LVIS minival æ ‡æ³¨æ–‡ä»¶
# å®˜æ–¹åœ°å€: https://www.lvisdataset.org/dataset
# æ”¾ç½®ä½ç½®: ../LVIS/minival/lvis_v1_minival.json

# ä¸‹è½½ COCO 2017 éªŒè¯é›†å›¾åƒ (~1GB)
# å®˜æ–¹åœ°å€: https://cocodataset.org/#download
# æ”¾ç½®ä½ç½®: ../LVIS/minival/ (ç¬¦å·é“¾æ¥æˆ–å¤åˆ¶ç›¸å…³å›¾åƒ)
```

æ•°æ®é›†ç›®å½•ç»“æ„ï¼š
```
../LVIS/
â”œâ”€â”€ minival/
â”‚   â”œâ”€â”€ lvis_v1_minival.json     # LVIS minival æ ‡æ³¨
â”‚   â””â”€â”€ *.jpg                     # COCO val2017 å›¾åƒ
â””â”€â”€ lvis_v1_val.json             # (å¯é€‰) å®Œæ•´ LVIS éªŒè¯é›†æ ‡æ³¨
```

**æ³¨æ„**: minival æ˜¯ LVIS éªŒè¯é›†çš„å­é›† (4,752 å¼ å›¾åƒ)ï¼Œæ’é™¤äº†ä¸ COCO 2017 è®­ç»ƒé›†é‡å çš„æ ·æœ¬ï¼Œç”¨äºå…¬å¹³è¯„ä¼°ã€‚

## Quick Start

### Inference Demo

```bash
# Demo mode (ä½¿ç”¨å†…ç½®æµ‹è¯•å›¾åƒ)
python scripts/run_inference.py --demo

# Custom image (è‡ªå®šä¹‰å›¾åƒå’Œæ–‡æœ¬)
python scripts/run_inference.py \
    --image your_image.jpg \
    --text "cat . dog . person ." \
    --output result.jpg \
    --box_threshold 0.3
```

---

## ğŸ”¬ å®éªŒè¿è¡ŒæŒ‡å—

### 1. Zero-Shot è¯„ä¼° (LVIS)

ä½¿ç”¨ `eval_lvis_zeroshot_full.py` è¿›è¡Œé›¶æ ·æœ¬æ£€æµ‹è¯„ä¼°ï¼š

```bash
# å¿«é€Ÿæµ‹è¯• (100 å¼ å›¾åƒ, ~5 åˆ†é’Ÿ)
python scripts/eval_lvis_zeroshot_full.py \
    --num_images 100 \
    --gpu 0 \
    --output_dir outputs/zeroshot_test

# å®Œæ•´ LVIS minival è¯„ä¼° (4752 å¼ å›¾åƒ, ~7 å°æ—¶)
python scripts/eval_lvis_zeroshot_full.py \
    --full \
    --gpu 0 \
    --output_dir outputs/zeroshot_full

# ä½¿ç”¨è¶…ä¼˜åŒ–æ¨¡å¼ (å‡å°‘ GPU-CPU åŒæ­¥, æå‡ 15-25%)
python scripts/eval_lvis_zeroshot_full.py \
    --full \
    --ultra_optimized \
    --gpu 0

# æ–­ç‚¹ç»­ä¼  (ä»ä¸­æ–­å¤„ç»§ç»­)
python scripts/eval_lvis_zeroshot_full.py \
    --full \
    --resume \
    --output_dir outputs/zeroshot_full
```

**ä¸»è¦å‚æ•°è¯´æ˜**:
| å‚æ•° | è¯´æ˜ | é»˜è®¤å€¼ |
|------|------|--------|
| `--checkpoint` | Jittor æƒé‡è·¯å¾„ | `weights/groundingdino_swint_ogc_jittor.pkl` |
| `--num_images` | è¯„ä¼°å›¾åƒæ•°é‡ | 100 |
| `--full` | è¯„ä¼°å®Œæ•´ minival | False |
| `--batch_size` | ç±»åˆ«æ‰¹å¤§å° (BERT token é™åˆ¶) | 60 |
| `--box_threshold` | ç½®ä¿¡åº¦é˜ˆå€¼ | 0.1 |
| `--ultra_optimized` | å¯ç”¨è¶…ä¼˜åŒ–æ¨¡å¼ | False |
| `--resume` | æ–­ç‚¹ç»­ä¼  | False |
| `--checkpoint_interval` | ä¿å­˜æ£€æŸ¥ç‚¹é—´éš” | 250 |

### 2. å¾®è°ƒ (Fine-tuning)

ä½¿ç”¨ `finetune_lvis_v2.py` è¿›è¡Œ LVIS å¾®è°ƒï¼š

```bash
# å¿«é€Ÿæµ‹è¯• (éªŒè¯è®­ç»ƒæµç¨‹)
python scripts/finetune_lvis_v2.py \
    --test_only \
    --num_samples 10 \
    --epochs 2

# å°è§„æ¨¡å¾®è°ƒ (100 æ ·æœ¬, 5 epochs)
python scripts/finetune_lvis_v2.py \
    --num_samples 100 \
    --epochs 5 \
    --batch_size 2 \
    --gradient_accumulation 16 \
    --lr 1e-4 \
    --lr_backbone 1e-5 \
    --output_dir outputs/finetune_100

# å¤§è§„æ¨¡å¾®è°ƒ (1000 æ ·æœ¬, æ¨èé…ç½®)
python scripts/finetune_lvis_v2.py \
    --num_samples 1000 \
    --epochs 24 \
    --batch_size 4 \
    --gradient_accumulation 4 \
    --lr 1e-4 \
    --lr_backbone 1e-5 \
    --freeze_text_encoder \
    --output_dir outputs/finetune_1k
```

**å¾®è°ƒå‚æ•°è¯´æ˜**:
| å‚æ•° | è¯´æ˜ | é»˜è®¤å€¼ |
|------|------|--------|
| `--checkpoint` | é¢„è®­ç»ƒæƒé‡è·¯å¾„ | Jittor æƒé‡ |
| `--num_samples` | è®­ç»ƒæ ·æœ¬æ•°é‡ | 100 |
| `--epochs` | è®­ç»ƒè½®æ•° | 24 |
| `--batch_size` | æ‰¹å¤§å° | 4 |
| `--gradient_accumulation` | æ¢¯åº¦ç´¯ç§¯æ­¥æ•° | 4 |
| `--lr` | å­¦ä¹ ç‡ | 1e-4 |
| `--lr_backbone` | éª¨å¹²ç½‘ç»œå­¦ä¹ ç‡ | 1e-5 |
| `--freeze_text_encoder` | å†»ç»“ BERT | True |
| `--freeze_backbone` | å†»ç»“ Swin-T | False |
| `--clip_grad_norm` | æ¢¯åº¦è£å‰ª | 0.1 |

### 3. è¯„ä¼°å¾®è°ƒåæ¨¡å‹

```bash
# ä½¿ç”¨å¾®è°ƒæƒé‡è¿›è¡Œè¯„ä¼°
python scripts/eval_lvis_zeroshot_full.py \
    --finetuned_checkpoint outputs/finetune_1k/checkpoint_best.pkl \
    --base_checkpoint weights/groundingdino_swint_ogc_jittor.pkl \
    --num_images 100 \
    --output_dir outputs/eval_finetuned
```

### 4. æ¶ˆèå®éªŒ (Ablation)

ä½¿ç”¨ `eval_Gdino_ablation.py` éªŒè¯å…³é”®ç»„ä»¶çš„ä½œç”¨ï¼š

```bash
# æ¶ˆè1: ç§»é™¤æ–‡æœ¬äº¤å‰æ³¨æ„åŠ›
python scripts/eval_Gdino_ablation.py \
    --ablation no_text_cross_attn \
    --num_images 100 \
    --output_dir outputs/ablation_no_cross_attn

# æ¶ˆè2: éšæœºæ–‡æœ¬åµŒå…¥
python scripts/eval_Gdino_ablation.py \
    --ablation random_text \
    --num_images 100 \
    --output_dir outputs/ablation_random_text

# å®Œæ•´æ¶ˆèå®éªŒ (4752 å¼ å›¾åƒ)
python scripts/eval_Gdino_ablation.py \
    --ablation no_text_cross_attn \
    --full \
    --output_dir outputs/ablation_full
```

### 5. OWL-ViT å¯¹æ¯”å®éªŒ

ä½¿ç”¨ `eval_owlvit_lvis.py` ä¸ OWL-ViT è¿›è¡Œå¯¹æ¯”ï¼š

```bash
# å¿«é€Ÿå¯¹æ¯” (100 å¼ å›¾åƒ)
python scripts/eval_owlvit_lvis.py \
    --num_images 100 \
    --batch_size 25 \
    --output_dir outputs/owlvit_test

# å®Œæ•´å¯¹æ¯” (4752 å¼ å›¾åƒ)
python scripts/eval_owlvit_lvis.py \
    --full \
    --batch_size 25 \
    --resume \
    --output_dir outputs/owlvit_full
```

### 6. ç»“æœå¯è§†åŒ–

```bash
# å¯è§†åŒ–æ£€æµ‹ç»“æœ
python scripts/visualize_lvis_predictions.py \
    --predictions outputs/zeroshot_full/lvis_predictions.json \
    --lvis_ann ../LVIS/minival/lvis_v1_minival.json \
    --image_dir ../LVIS/minival \
    --output_dir outputs/visualizations \
    --score_threshold 0.3 \
    --max_boxes 50
```

## Evaluation Scripts Overview

| Script | Purpose | Categories | Speed |
|--------|---------|------------|-------|
| `eval_lvis_zeroshot_full.py` | Official benchmarking | All 1203 | ~5.5s/img |
| `quick_test_zeroshot.py` | Debugging & visualization | GT only | ~0.3s/img |
| `eval_owlvit_lvis.py` | OWL-ViT comparison | All 1203 | ~2.4s/img |
| `eval_Gdino_ablation.py` | Ablation studies | All 1203 | ~5.5s/img |
| `finetune_lvis_v2.py` | LVIS fine-tuning | All 1203 | - |
| `visualize_lvis_predictions.py` | Result visualization | - | - |

## Output Files

è¯„ä¼°å’Œè®­ç»ƒè„šæœ¬ä¼šç”Ÿæˆä»¥ä¸‹è¾“å‡ºæ–‡ä»¶ï¼š

### Zero-Shot è¯„ä¼°è¾“å‡º
```
outputs/zeroshot_full/
â”œâ”€â”€ predictions.jsonl          # é€è¡Œ JSON é¢„æµ‹ç»“æœ (æ”¯æŒæ–­ç‚¹ç»­ä¼ )
â”œâ”€â”€ progress.json              # æ–­ç‚¹ç»­ä¼ è¿›åº¦
â”œâ”€â”€ lvis_predictions.json      # å®Œæ•´é¢„æµ‹ç»“æœ (LVIS æ ¼å¼)
â”œâ”€â”€ lvis_zeroshot_results.json # è¯„ä¼°æŒ‡æ ‡æ±‡æ€»
â””â”€â”€ eval.log                   # è¿è¡Œæ—¥å¿—
```

### å¾®è°ƒè¾“å‡º
```
outputs/finetune_1k/
â”œâ”€â”€ checkpoint_epoch_XX.pkl    # å„ epoch æ£€æŸ¥ç‚¹
â”œâ”€â”€ checkpoint_best.pkl        # æœ€ä½³æ¨¡å‹ (æŒ‰éªŒè¯ AP)
â”œâ”€â”€ training_log.json          # è®­ç»ƒæŸå¤±æ›²çº¿
â””â”€â”€ config.json                # è®­ç»ƒé…ç½®
```

## Performance Optimization

| Optimization | Before | After | Speedup |
|--------------|--------|-------|---------|
| Vision feature caching | 14.8s/img | 5.5s/img | **63%** |
| Vectorized post-processing | - | - | included |
| Category batching (60/batch) | OOM | stable | - |

## Troubleshooting

### å¸¸è§é—®é¢˜

**1. BERT æ¨¡å‹åŠ è½½å¤±è´¥**
```bash
# ç¡®ä¿ transformers å·²å®‰è£…
pip install transformers

# é¦–æ¬¡è¿è¡Œéœ€è¦è”ç½‘ä¸‹è½½ï¼Œä¹‹åå¯ä»¥è®¾ç½®ç¦»çº¿æ¨¡å¼
export HF_HUB_OFFLINE=1
```

**2. CUDA å†…å­˜ä¸è¶³ (OOM)**
```bash
# å‡å° batch_size
python scripts/eval_lvis_zeroshot_full.py --batch_size 30

# æˆ–ä½¿ç”¨ CPU æ¨¡å¼
CUDA_VISIBLE_DEVICES="" python scripts/eval_lvis_zeroshot_full.py
```

**3. JIT ç¼–è¯‘å†²çª (å¤š GPU)**
```bash
# é™åˆ¶ JIT å¹¶è¡Œç¼–è¯‘æ•°
export JT_COMPILE_PARALLEL=1
```

**4. æƒé‡è½¬æ¢å¤±è´¥**
```bash
# ç¡®ä¿åŒæ—¶å®‰è£…äº† PyTorch å’Œ Jittor
pip install torch
pip install jittor
```

**5. å›¾åƒè·¯å¾„ä¸åŒ¹é…**
```bash
# æ£€æŸ¥å›¾åƒç›®å½•ç»“æ„
ls ../LVIS/minival/*.jpg | head -5

# å¦‚æœå›¾åƒåœ¨å­ç›®å½•ä¸­ï¼Œä½¿ç”¨è½¯é“¾æ¥
ln -s /path/to/coco/val2017/*.jpg ../LVIS/minival/
```

## Citation

```bibtex
@inproceedings{liu2023grounding,
  title={Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection},
  author={Liu, Shilong and Zeng, Zhaoyang and Ren, Tianhe and Li, Feng and Zhang, Hao and Yang, Jie and Li, Chunyuan and Yang, Jianwei and Su, Hang and Zhu, Jun and Zhang, Lei},
  booktitle={European Conference on Computer Vision},
  year={2024}
}
```

## Team

- å¼ æ¯… (2022010387, å·¥22) - grounding-dinoçš„Jittorå¤ç°åŠå…¶ä»–è„šæœ¬æ’°å†™
- æ¨å¼˜æ¯… (2023011638, è‹±31) - zero-shotåŠæ¨¡å‹å¯¹æ¯”å’Œæ¶ˆèå®éªŒç­‰é¢å¤–ä»»åŠ¡
- è‹åšå®‡ (2023011277, ç‰©ç†32) - å¾®è°ƒåŠè®­ç»ƒpipeline

## License

This project is for educational purposes as part of the Tsinghua University ANN course final project.
