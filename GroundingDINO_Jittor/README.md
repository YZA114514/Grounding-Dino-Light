# GroundingDINO Jittor Implementation

This project is a Jittor implementation of GroundingDINO, as part of the 2025 Final Project.

## ğŸ¯ Zero-Shot Evaluation Results

Our Jittor implementation achieves comparable performance to the official PyTorch implementation on LVIS zero-shot object detection:

| Metric | Our Result | Paper Target | Status |
|--------|-----------|--------------|--------|
| **AP** | 23.5% | 25.6% | âœ… Close |
| **APr** (rare) | 16.7% | 14.4% | âœ… Exceeded |
| **APc** (common) | 18.0% | 19.6% | âœ… Close |
| **APf** (frequent) | 24.1% | 32.2% | âš ï¸ In progress |

*Results on 100 images with true zero-shot evaluation (all 1203 LVIS categories)*

## Project Structure

The project structure is organized based on the roles and responsibilities defined in the team plan:

```
GroundingDINO_Jittor/
â”œâ”€â”€ jittor_implementation/        # æ ¸å¿ƒä»£ç åº“
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ models/                   # [æˆå‘˜A] æ¨¡å‹æ¶æ„
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ backbone/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â””â”€â”€ swin_transformer.py
â”‚   â”‚   â”œâ”€â”€ attention/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â””â”€â”€ ms_deform_attn.py
â”‚   â”‚   â”œâ”€â”€ transformer/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ encoder.py
â”‚   â”‚   â”‚   â””â”€â”€ decoder.py
â”‚   â”‚   â”œâ”€â”€ head/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â””â”€â”€ dino_head.py
â”‚   â”‚   â”œâ”€â”€ text_encoder/         # [æˆå‘˜C] æ–‡æœ¬ç¼–ç 
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ bert_wrapper.py
â”‚   â”‚   â”‚   â””â”€â”€ text_processor.py
â”‚   â”‚   â”œâ”€â”€ fusion/               # [æˆå‘˜C] ç‰¹å¾èåˆ
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â””â”€â”€ feature_fusion.py
â”‚   â”‚   â”œâ”€â”€ query/                # [æˆå‘˜C] Queryç”Ÿæˆ
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â””â”€â”€ language_guided_query.py
â”‚   â”‚   â”œâ”€â”€ groundingdino.py      # [æˆå‘˜A] å®Œæ•´æ¨¡å‹ç»„è£…
â”‚   â”‚   â””â”€â”€ interfaces.py         # [å…¨ä½“] æ¥å£å®šä¹‰
â”‚   â”œâ”€â”€ data/                     # [æˆå‘˜B] æ•°æ®å¤„ç†
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ transforms.py         # æ•°æ®é¢„å¤„ç†
â”‚   â”‚   â”œâ”€â”€ dataset.py            # æ•°æ®é›†åŠ è½½ (LVISDatasetç­‰)
â”‚   â”‚   â””â”€â”€ sampler.py            # é‡‡æ ·ç­–ç•¥
â”‚   â”œâ”€â”€ losses/                   # [æˆå‘˜B] æŸå¤±å‡½æ•°
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ focal_loss.py
â”‚   â”‚   â”œâ”€â”€ giou_loss.py
â”‚   â”‚   â”œâ”€â”€ l1_loss.py
â”‚   â”‚   â””â”€â”€ grounding_loss.py
â”‚   â”œâ”€â”€ eval/                     # [æˆå‘˜B] è¯„ä¼°
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ lvis_evaluator.py
â”‚   â”œâ”€â”€ train/                    # [æˆå‘˜C] è®­ç»ƒç›¸å…³
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ trainer.py
â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â””â”€â”€ utils.py
â”‚   â””â”€â”€ experiments/              # [æˆå‘˜C] å®éªŒ
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ vlm_comparison.py
â”œâ”€â”€ scripts/                      # å·¥å…·è„šæœ¬
â”‚   â”œâ”€â”€ convert_weights_pytorch_to_jittor.py # æƒé‡è½¬æ¢
â”‚   â”œâ”€â”€ eval_lvis_zeroshot_full.py  # LVIS Zero-Shot å®Œæ•´è¯„ä¼°
â”‚   â”œâ”€â”€ quick_test_zeroshot.py      # å¿«é€Ÿæ¨ç†æµ‹è¯•
â”‚   â”œâ”€â”€ run_inference.py            # æ¨ç†è„šæœ¬
â”‚   â”œâ”€â”€ finetune.py                 # å¾®è°ƒè„šæœ¬
â”‚   â”œâ”€â”€ coco2odvg.py                # COCOæ ¼å¼è½¬æ¢
â”‚   â””â”€â”€ goldg2odvg.py               # GoldGæ ¼å¼è½¬æ¢
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## Evaluation Scripts Comparison

This project includes several evaluation scripts with different purposes and trade-offs:

### Script Overview

| Script | Purpose | Zero-Shot | Speed | Use Case |
|--------|----------|-----------|-------|----------|
| `eval_lvis_zeroshot_full.py` | Official benchmarking | âœ“ True (all 1203 cats) | Slow (~1-5s/img) | Research, papers |
| `quick_test_zeroshot.py` | Development/testing | âœ— Uses GT | Fast (~0.1-0.5s/img) | Debugging, visualization |
| `eval_lvis_zeroshot.py` | Alternative evaluation | âœ— Uses GT | Medium | Development |
| `eval_lvis_zeroshot_final.py` | Debug version | âœ— Partial (25 cats) | Medium | Token mapping debugging |

### Key Differences

#### 1. Category Handling

**`eval_lvis_zeroshot_full.py` (True Zero-Shot)**
- Processes ALL 1203 LVIS categories in batches (default 80 per batch)
- Uses PyTorch's `build_captions_and_token_span()` for proper token mapping
- Multiple forward passes per image (~15 for full evaluation)
- Results comparable to Grounding DINO paper

**`quick_test_zeroshot.py` (Non-Zero-Shot)**
- Uses ONLY ground truth categories from each image
- Typically 2-10 categories per image
- Single forward pass per image
- Good for quick sanity checks but NOT for benchmarking

#### 2. Token-to-Category Mapping

**`eval_lvis_zeroshot_full.py`**
```python
# Uses positive map matrix from PyTorch utilities
positive_map = create_positive_map_from_span(tokenized, tokenspanlist, max_text_len)
prob_to_label = prob_to_token @ positive_map_np.T
```

**`quick_test_zeroshot.py`**
```python
# Simple argmax approach
pred_probs = jt.sigmoid(pred_logits)
max_probs, pred_labels = jt.argmax(pred_probs, dim=-1)
```

#### 3. Evaluation Method

**`eval_lvis_zeroshot_full.py`**
- Official COCO/LVIS evaluation
- Full metric suite: AP, AP50, AP75, APs, APm, APl, APr, APc, APf
- Reproducible and comparable with paper results

**`quick_test_zeroshot.py`**
- Custom IoU-based TP calculation
- Simple precision/recall/F1
- Includes visualization of bounding boxes

### When to Use Which Script?

#### Use `eval_lvis_zeroshot_full.py` when:
- âœ“ Running official benchmarks for research papers
- âœ“ Comparing with Grounding DINO paper metrics
- âœ“ Need all COCO/LVIS metrics

#### Use `quick_test_zeroshot.py` when:
- âœ“ Debugging model inference
- âœ“ Visualizing predictions on sample images
- âœ“ Quick sanity checks during development
- âœ“ Testing model loading and basic functionality
- âœ“ Verifying output format

### Performance Characteristics

| Metric | eval_lvis_zeroshot_full | quick_test_zeroshot |
|--------|-------------------------|---------------------|
| **Categories processed** | 1203 | ~5 (GT only) |
| **Forward passes/image** | ~15 | 1 |
| **Memory usage** | Higher | Lower |
| **Time per image** | 1-5 seconds | 0.1-0.5 seconds |
| **Total time (100 images)** | ~2-8 minutes | ~10-50 seconds |
| **Visualization** | No | Yes |
| **Official metrics** | Yes | No |

### OWL-ViT Comparison Script

**`eval_owlvit_lvis.py`** - Compare with OWL-ViT baseline model

This script evaluates Google's OWL-ViT model on the same LVIS dataset for direct performance comparison with Grounding DINO.

#### Key Features:
- Uses HuggingFace `transformers` library for OWL-ViT
- Processes same LVIS minival dataset (1203 categories)
- Generates identical output format and metrics as GroundingDINO
- Enables direct quantitative comparison (AP, APr, APc, APf)

#### Usage:
```bash
# Quick test (100 images)
python scripts/eval_owlvit_lvis.py --num_images 100 --batch_size 25

# Full evaluation
python scripts/eval_owlvit_lvis.py --full --batch_size 25

# Custom model variant
python scripts/eval_owlvit_lvis.py \
    --model_name 'google/owlvit-large-patch14' \
    --num_images 500 \
    --output_dir outputs/owlvit_large
```

#### Test Setup:
```bash
# Verify installation and data access
python scripts/test_owlvit_quick.py
```

#### Requirements:
- `transformers >= 4.20.0`
- `torch >= 1.13.0`
- `torchvision >= 0.14.0`
- LVIS dataset (same as GroundingDINO evaluation)

#### Output:
- `outputs/owlvit/predictions.jsonl` - Incremental predictions
- `outputs/owlvit/lvis_predictions.json` - Final predictions for LVISEval
- `outputs/owlvit/lvis_zeroshot_results.json` - Metrics comparable to GroundingDINO

#### When to Use:
- âœ“ Benchmarking against OWL-ViT baseline
- âœ“ VLM performance comparison studies
- âœ“ Understanding open-vocabulary detection capabilities
- âœ“ Research requiring multiple model comparisons

### Example Usage

```bash
# True zero-shot evaluation (use for benchmarks)
python scripts/eval_lvis_zeroshot_full.py --num_images 100 --gpu 0

# Quick testing with visualization (use for debugging)
python scripts/quick_test_zeroshot.py \
    --num_images 10 \
    --output_dir outputs/quick_test \
    --box_threshold 0.1
```

## Installation

### å‰ç½®è¦æ±‚

- Anaconda æˆ– Miniconda (æ¨èä½¿ç”¨ conda ç®¡ç†ç¯å¢ƒ)
- Python 3.9
- CUDA (å¯é€‰ï¼Œç”¨äº GPU åŠ é€Ÿ)

### å¿«é€Ÿå®‰è£… (æ¨èæ–¹æ³•)

**æ³¨æ„**: è¯·ä½¿ç”¨é¡¹ç›®æ ¹ç›®å½• `GroundingDINO-Light/.venv` ä¸­çš„è™šæ‹Ÿç¯å¢ƒï¼Œè€Œä¸æ˜¯ `GroundingDINO_Jittor/.venv`ã€‚

å¦‚æœ conda åˆ›å»ºç¯å¢ƒå¾ˆæ…¢ï¼Œå»ºè®®ç›´æ¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ï¼š

```bash
# 1. ä½¿ç”¨é¡¹ç›®æ ¹ç›®å½•çš„è™šæ‹Ÿç¯å¢ƒ
cd ..  # è¿”å›åˆ° GroundingDINO-Light æ ¹ç›®å½•
source .venv/bin/activate  # ä½¿ç”¨æ ¹ç›®å½•çš„ .venv

# 2. è¿›å…¥ Jittor é¡¹ç›®ç›®å½•
cd GroundingDINO_Jittor

# 3. ä½¿ç”¨ pip å®‰è£…æ‰€æœ‰ä¾èµ– (æ›´å¿«)
pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple

# 4. éªŒè¯å®‰è£…
python -c "import jittor as jt; print(f'Jittor: {jt.__version__}')"
python -c "import torch, transformers, timm, pycocotools; print('æ‰€æœ‰ä¾èµ–å®‰è£…æˆåŠŸ!')"
```

### ä½¿ç”¨ Conda ç¯å¢ƒæ–‡ä»¶ (è¾ƒæ…¢)

å¦‚æœç½‘ç»œè¾ƒå¥½ï¼Œå¯ä»¥ä½¿ç”¨ï¼š

```bash
# é…ç½®å›½å†…é•œåƒæº (åŠ é€Ÿ)
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free

# åˆ›å»ºç¯å¢ƒ
conda env create -f environment.yml

# æ¿€æ´»ç¯å¢ƒ
conda activate groundingdino_jittor
```

### ä¸»è¦ä¾èµ–

- **jittor** >= 1.3.0 - æ ¸å¿ƒæ·±åº¦å­¦ä¹ æ¡†æ¶
- **torch** >= 1.13.0 - ç”¨äº BERT æ¨¡å‹å’Œæƒé‡è½¬æ¢
- **transformers** >= 4.20.0 - BERT æ–‡æœ¬ç¼–ç å™¨
- **timm** >= 0.6.0 - Swin Transformer backbone
- **pycocotools** >= 2.0.4 - LVIS/COCO è¯„ä¼°
- numpy, pillow, matplotlib - æ•°æ®å¤„ç†å’Œå¯è§†åŒ–

### å¸¸è§é—®é¢˜

- **conda å‘½ä»¤æ‰¾ä¸åˆ°**: ä½¿ç”¨ Anaconda Prompt (Windows) æˆ–é‡å¯ç»ˆç«¯
- **ç¯å¢ƒåˆ›å»ºå¤±è´¥**: æ£€æŸ¥ç½‘ç»œè¿æ¥ï¼Œæˆ–æ‰‹åŠ¨åˆ›å»ºç¯å¢ƒåä½¿ç”¨ `pip install -r requirements.txt`
- **GPU æ”¯æŒ**: Jittor ä¼šè‡ªåŠ¨æ£€æµ‹ CUDAï¼Œæ— éœ€æ‰‹åŠ¨é…ç½®

## Quick Start - Inference

### 1. ä¸‹è½½é¢„è®­ç»ƒæƒé‡

ä»å®˜æ–¹ GitHub ä¸‹è½½ PyTorch é¢„è®­ç»ƒæƒé‡ï¼š

```bash
# åˆ›å»º weights ç›®å½•
mkdir weights
cd weights

# ä¸‹è½½ Swin-T ç‰ˆæœ¬æƒé‡ (~694MB)
# æ–¹æ³•1: ä½¿ç”¨ wget (Linux/Mac)
wget https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth

# æ–¹æ³•2: ä½¿ç”¨æµè§ˆå™¨ç›´æ¥ä¸‹è½½
# è®¿é—®: https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth

cd ..
```

### 2. è½¬æ¢æƒé‡åˆ° Jittor æ ¼å¼

```bash
python scripts/convert_weights_pytorch_to_jittor.py \
    --pytorch_weight weights/groundingdino_swint_ogc.pth \
    --output weights/groundingdino_swint_ogc_jittor.pkl
```

è½¬æ¢æˆåŠŸåä¼šæ˜¾ç¤ºï¼š
```
æˆåŠŸåŠ è½½ 940 ä¸ªæƒé‡
æˆåŠŸä¿å­˜ 940 ä¸ªæƒé‡
è½¬æ¢å®Œæˆï¼
```
### ä¸‹è½½bertæ¨¡å‹æ”¾åœ¨Grounding-Dino-Light/GroundingDINO_Jittor/models
### ä¸‹è½½æ•°æ®åˆ°Grounding-Dino-Light/GroundingDINO_Jittor/data/coco/val2017ï¼›Grounding-Dino-Light/GroundingDINO_Jittor/data/lvis_notation

### 3. è¿è¡Œæ¨ç†

#### æ¼”ç¤ºæ¨¡å¼ï¼ˆè‡ªåŠ¨åˆ›å»ºæµ‹è¯•å›¾åƒï¼‰

```bash
python scripts/run_inference.py --demo
```

#### è‡ªå®šä¹‰å›¾åƒæ¨ç†

```bash
python scripts/run_inference.py \
    --image your_image.jpg \
    --text "cat . dog . person ." \
    --output result.jpg
```

#### å®Œæ•´å‚æ•°

```bash
python scripts/run_inference.py \
    --image <å›¾åƒè·¯å¾„> \
    --text <æ–‡æœ¬æç¤ºï¼Œç”¨ . åˆ†éš”ä¸åŒç±»åˆ«> \
    --output <è¾“å‡ºè·¯å¾„> \
    --box_threshold 0.35 \
    --text_threshold 0.25
```

### LVIS Zero-Shot Evaluation

Run the full zero-shot evaluation on LVIS dataset:

```bash
# Quick test on 100 images
python scripts/eval_lvis_zeroshot_full.py --num_images 100 --gpu 0

# Full validation set (~17K images, ~85 hours)
python scripts/eval_lvis_zeroshot_full.py --full --gpu 0

# Custom parameters
python scripts/eval_lvis_zeroshot_full.py \
    --num_images 500 \
    --batch_size 80 \
    --checkpoint weights/groundingdino_swint_ogc_jittor.pkl \
    --lvis_ann data/lvis_notation/lvis_v1_val.json/lvis_v1_val.json \
    --image_dir data/coco/val2017 \
    --output_dir outputs
```

### LVIS Fine-tuning

Fine-tune Grounding DINO on LVIS dataset to achieve **AP 52.1** (target from paper):

```bash
# Quick test (verify script works)
python scripts/finetune_lvis_full.py --test_only --num_samples 10 --epochs 2 --gpu 0

# Full fine-tuning (recommended settings from paper)
python scripts/finetune_lvis_full.py \
    --epochs 20 \
    --batch_size 4 \
    --lr 1e-4 \
    --lr_backbone 1e-5 \
    --lr_drop 15 \
    --output_dir outputs/finetune_lvis \
    --gpu 0

# With frozen backbone (faster, less memory)
python scripts/finetune_lvis_full.py \
    --epochs 20 \
    --batch_size 8 \
    --freeze_backbone \
    --output_dir outputs/finetune_frozen_backbone \
    --gpu 0
```

**Fine-tuning Targets:**

| Metric | Target |
|--------|--------|
| AP | 52.1% |
| APr (rare) | 35.4% |
| APc (common) | 51.3% |
| APf (frequent) | 55.7% |

**Training Notes:**
- Full training on LVIS (~100K images) takes approximately 40-60 hours on a single GPU
- Recommended: Use multi-GPU training or freeze backbone to reduce training time
- Learning rate drops by 10x at epoch 15 (configurable via `--lr_drop`)
- Checkpoints saved every 5 epochs and at best validation loss

### æ¨ç†ç¤ºä¾‹

```python
from jittor_implementation.util.inference import GroundingDINOInference

# åˆå§‹åŒ–æ¨¡å‹
model = GroundingDINOInference(
    weight_path="weights/groundingdino_swint_ogc_jittor.pkl",
    device="cuda",
    box_threshold=0.35,
    text_threshold=0.25,
)

# æ‰§è¡Œæ¨ç†
boxes, scores, phrases = model.predict(
    image="path/to/image.jpg",
    caption="cat . dog . person ."
)

# æ¨ç†å¹¶å¯è§†åŒ–
result_image = model.predict_and_visualize(
    image_path="path/to/image.jpg",
    caption="cat . dog . person .",
    output_path="output.jpg"
)
```

---

## Usage

### Data Loading

```python
from jittor_implementation.data import build_dataset, get_dataloader

# Build dataset
dataset = build_dataset('train', args)

# Create dataloader with LVIS sampler
dataloader = get_dataloader(
    dataset, 
    batch_size=4, 
    sampler_type='lvis',
    sampler_kwargs={'samples_per_epoch': 1000}
)
```

### Loss Functions

```python
from jittor_implementation.losses import GroundingLoss, SetCriterion

# Create loss function
criterion = SetCriterion(
    num_classes=1203,
    weight_dict={'loss_ce': 2.0, 'loss_bbox': 5.0, 'loss_giou': 2.0},
    losses=['labels', 'boxes', 'giou']
)

# Calculate loss
outputs = model(images)
losses = criterion(outputs, targets)
```

### Evaluation

```python
from jittor_implementation.eval import evaluate_lvis

# Evaluate model
metrics = evaluate_lvis(
    model, 
    dataloader, 
    ann_file='path/to/lvis_val.json',
    output_dir='./eval_results'
)

print(f"AP: {metrics['AP']:.4f}")
```

### Data Format Conversion

```bash
# Convert COCO to ODVG
python scripts/coco2odvg.py --coco_path path/to/coco.json --output_path path/to/odvg.json --image_dir path/to/images

# Convert GoldG to ODVG
python scripts/goldg2odvg.py --goldg_path path/to/goldg.json --output_path path/to/odvg.json --image_dir path/to/images
```

### Text Encoding

```python
from jittor_implementation.models.text_encoder import BERTWrapper

# Initialize text encoder
text_encoder = BERTWrapper(
    model_name='bert-base-uncased',
    max_text_len=256
)

# Process text
text = ["person . dog . cat"]
text_dict = text_encoder(text)

# Access encoded features
encoded_text = text_dict["encoded_text"]  # (B, L, D)
text_token_mask = text_dict["text_token_mask"]  # (B, L)
position_ids = text_dict["position_ids"]  # (B, L)
```

### Feature Fusion

```python
from jittor_implementation.models.fusion import FeatureFusion

# Initialize fusion module
fusion = FeatureFusion(
    hidden_dim=256,
    num_heads=8,
    dropout=0.1
)

# Fuse visual and text features
fused_features = fusion(
    visual_features,  # (B, H, W, D) or (B, N, D)
    text_features,    # (B, L, D)
    text_token_mask   # (B, L)
)
```

### Query Generation

```python
from jittor_implementation.models.query import LanguageGuidedQuery

# Initialize query generator
query_generator = LanguageGuidedQuery(
    hidden_dim=256,
    num_queries=900
)

# Generate queries from text
queries = query_generator(
    text_features,  # (B, L, D)
    text_token_mask  # (B, L)
)
```

### Training

```python
from jittor_implementation.train.config import TrainingConfig
from jittor_implementation.train.trainer import Trainer

# Create configuration
config = TrainingConfig()
config.model_name = "groundingdino_swin-t"
config.batch_size = 4
config.epochs = 40

# Create trainer
trainer = Trainer(
    model=model,
    text_encoder=text_encoder,
    train_loader=train_loader,
    val_loader=val_loader,
    criterion=criterion,
    optimizer=optimizer,
    scheduler=scheduler,
    config=config
)

# Start training
trainer.train()
```

### Command-line Training

```bash
# Train model with default configuration
python -m jittor_implementation.train.trainer \
  --model_name groundingdino_swin-t \
  --batch_size 4 \
  --epochs 40 \
  --lr 1e-4 \
  --lr_backbone 1e-5 \
  --data_path /path/to/dataset \
  --output_dir ./outputs \
  --checkpoint_dir ./checkpoints

# Resume training from checkpoint
python -m jittor_implementation.train.trainer \
  --model_name groundingdino_swin-t \
  --resume ./checkpoints/groundingdino_latest.pth \
  --data_path /path/to/dataset \
  --output_dir ./outputs \
  --checkpoint_dir ./checkpoints
```

### VLM Comparison

```python
from jittor_implementation.experiments.vlm_comparison import VLMComparator

# Initialize comparator
comparator = VLMComparator(
    model=model,
    text_encoder=text_encoder,
    config=config,
    output_dir="./comparison_results"
)

# Process images with text prompts
results = comparator.run_comparison(
    image_list=["image1.jpg", "image2.jpg"],
    text_prompts=["person", "dog", "cat"],
    save_visualizations=True
)
```

### Command-line VLM Comparison

```bash
# Compare model outputs on test images
python -m jittor_implementation.experiments.vlm_comparison \
  --checkpoint_path ./checkpoints/groundingdino_best.pth \
  --image_list image1.jpg image2.jpg image3.jpg \
  --text_prompts "person . dog" "car . bicycle" "cat . bird" \
  --output_dir ./comparison_results \
  --save_visualizations
```
```bash
# Start two gpu run on the whole LVIS/val dataset
 cd GroundingDINO_Jittor && source ../.venv/bin/activate && python scripts/eval_lvis_zeroshot_full.py --full --n_gpus 2 --checkpoint_interval 500 --image_dir ../val2017 --image_dir_fallback ../train2017 --output_dir outputs/lvis_full_2gpu --resume 2>&1 | tee lvis_eval_fixed.log
```
```bash
# new startup
cd GroundingDINO_Jittor && source ../.venv/bin/activate && python scripts/eval_lvis_zeroshot_full.py --num_images 10
```
```bash
# new ablation
source .venv/bin/activate && cd GroundingDINO_Jittor && python scripts/eval_Gdino_ablation.py --ablation no_text_cross_attn --num_images 10
```